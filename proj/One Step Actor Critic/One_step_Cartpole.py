# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p1Xwv-deXQST8-D8AuKjio2PUJPC3sN3
"""

import numpy as np
import gym
import matplotlib.pyplot as plt
from queue import PriorityQueue
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import gym
from torch.distributions import Categorical

env = gym.make('CartPole-v1')
class Model(nn.Module):
  def __init__(self, input_size, hidden_size, output_size) -> None:
      super(Model, self).__init__()
      self.fc1 = nn.Linear(input_size, hidden_size)
      self.relu = nn.ReLU(inplace=False)
      self.fc2 = nn.Linear(hidden_size, output_size)
  def forward(self, x):
    x = self.fc1(x)
    x = F.relu(x)
    out = self.fc2(x)
    return out
class Policy(nn.Module):
  def __init__(self, input_size, hidden_size, output_size) -> None:
      super(Policy, self).__init__()
      self.fc1 = nn.Linear(input_size, hidden_size)
      self.relu = nn.ReLU(inplace=False)
      self.fc2 = nn.Linear(hidden_size, output_size)
      self.soft = nn.Softmax()
  def forward(self, x, temp=1.0):
    x = self.fc1(x)
    x = F.relu(x)
    x = self.fc2(x)
    x = x/temp
    out = F.softmax(x)
    return out

def phi(obs,M, sin_cos = 1):
  x = obs[0]
  v = obs[1]
  wt = obs[2]
  wtv = obs[3]
  M = M+1
  x1 = (x+2.4)/4.8
  v1 = (v+1.62)/(2*1.62) #(v+ np.pi/15)/(2*np.pi/15)#
  wt1 = (wt+ np.pi/15)/(2*np.pi/15)
  wtv1 = (wtv+2.75)/(2*2.75) #(wtv+ np.pi/15)/(2*np.pi/15)#

  if(sin_cos == 0):
    vec = np.array([np.cos(np.pi*x1*np.arange(1,M)), np.cos(np.pi*v1*np.arange(1,M)), np.cos(np.pi*wt1*np.arange(1,M)), np.cos(np.pi*wtv1*np.arange(1,M))])
  else:
    x1 = x/4.8
    v1 = v/(2*1.62)
    wt1= wt/(2*np.pi/15)
    wtv1 = wtv/(2*2.75)
    vec = np.array([np.sin(np.pi*x1*np.arange(1,M)), np.sin(np.pi*v1*np.arange(1,M)), np.sin(np.pi*wt1*np.arange(1,M)), np.sin(np.pi*wtv1*np.arange(1,M))])


  phi_s = np.append(np.array([1]), vec.flatten())
  return phi_s

reward_list = []
no_of_actions = []
for i in range(20):
  M = 5

  if torch.cuda.is_available():
      DEVICE = torch.device("cuda")
  else:
      DEVICE = torch.device("cpu")

  value_model = Model(4*M+1,128,1).to(DEVICE)
  pi_ = Policy(4*M+1,128,2).to(DEVICE)


  done = False
  gm = 0.99
  i = gm
  count = 0
  optimizer_val = optim.SGD(value_model.parameters(), lr = 0.001)
  optimizer_policy = optim.SGD(pi_.parameters(), lr = 0.001)



  rew = []
  list_itr = []
  time_step = 0
  temp = 2.0
  while(True):
    done = False
    i = 1
    loss1 = 0
    loss2 = 0
    r1 = 0
    #S0 = np.array([0.0,0,0,0])
    S0 = env.reset()
    #env.state = env.unwrapped.state = S0
    s_curr = S0
    while not done:
      #m = Categorical(pi_(torch.tensor(s_curr.tolist())))
      s_curr1 = phi(s_curr,M, sin_cos = 0)
      m = torch.from_numpy(s_curr1).float().unsqueeze(0).to(DEVICE)
      state = Categorical(pi_(m, temp = 1))
      action = state.sample()

      log_prob = state.log_prob(action)[0]

      time_step += 1

      # probs = pi_(torch.tensor(s_curr.tolist()))
      # action = random.choices([0,1], weights=probs, k=1)[0]

      # log_prob = torch.log(probs[action])
      #action = torch.argmax(probs).item()
      s_next, reward, done, info = env.step(action.item())
      s_next1 = phi(s_next,M, sin_cos = 0)

      m1 = torch.from_numpy(s_next1).float().unsqueeze(0).to(DEVICE)

      #delta = reward + gm*value_model(torch.tensor(s_next.tolist())) - value_model(torch.tensor(s_curr.tolist()))
      #loss_value =  F.mse_loss(reward + gm*value_model(torch.tensor(s_next.tolist())), value_model(torch.tensor(s_curr.tolist())))#i*delta**2
      delta = reward + gm*value_model(m1) - value_model(m)
      delta = delta.detach()
      #print(m1)
      #print(value_model(torch.tensor(s_curr.tolist()).to(DEVICE)))
      # k = gm*value_model(torch.tensor(m1))
      # k.detach()
      # i*F.mse_loss(reward + k, value_model(torch.tensor(m))) #
      loss_value = -i*delta* value_model(m)
      m.detach()
      m1.detach()


      optimizer_val.zero_grad()
      loss_value.backward()

      loss_policy = -i*delta*log_prob
      optimizer_policy.zero_grad()
      loss_policy.backward()

      optimizer_val.step()
      optimizer_policy.step()

      loss1 = loss_value + loss1
      loss2 = loss_policy + loss2

      i = i*gm
      s_curr = s_next
      #print(s_curr)
      #print(probs)
      r1 = r1+ reward

      #observation, reward, done, info = env.step(action)
    print("reward: ", r1, "loss_value: ", loss1, "loss_policy: ", loss2)
    rew.append(r1)
    list_itr.append(time_step)

    count = count+1
    print(count)
    if(count == 500):
      break;
    if count % 100 == 0:
      temp = temp/np.sqrt(2)

    #break;
  reward_list.append(rew)
  no_of_actions.append(list_itr)

avg_rew = np.mean(np.array(reward_list), axis = 0)
avg_no_of_actions = np.mean(np.array(no_of_actions), axis = 0)
print(avg_rew.shape)

import matplotlib.pyplot as plt


plt.figure(1)
plt.plot(avg_rew)
plt.ylabel('Reward')
plt.xlabel('episodes')
plt.title('Average Reward over 20 trails using One-step Actor-Critic')

plt.figure(2)
plt.plot(avg_no_of_actions, range(avg_no_of_actions.shape[0]))
plt.ylabel('episodes')
plt.xlabel('Timesteps')
plt.title('Avg Timesteps taken for episodes, using One-step Actor-Critic')

plt.show()