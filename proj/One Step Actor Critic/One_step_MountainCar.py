# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p1Xwv-deXQST8-D8AuKjio2PUJPC3sN3
"""

import numpy as np
import gym
import matplotlib.pyplot as plt
from queue import PriorityQueue
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import gym
from torch.distributions import Categorical

env = gym.make('MountainCar-v0')
class Model(nn.Module):
  def __init__(self, input_size, hidden_size, output_size) -> None:
      super(Model, self).__init__()
      self.fc1 = nn.Linear(input_size, hidden_size)
      self.relu = nn.ReLU(inplace=False)
      self.fc2 = nn.Linear(hidden_size, output_size)
  def forward(self, x):
    x = self.fc1(x)
    x = F.relu(x)
    out = self.fc2(x)
    return out
class Policy(nn.Module):
  def __init__(self, input_size, hidden_size, output_size) -> None:
      super(Policy, self).__init__()
      self.fc1 = nn.Linear(input_size, hidden_size)
      self.relu = nn.ReLU(inplace=False)
      self.fc2 = nn.Linear(hidden_size, output_size)
      self.soft = nn.Softmax()
  def forward(self, x, temp=1.0):
    x = self.fc1(x)
    x = F.relu(x)
    x = self.fc2(x)
    x = x/temp
    out = F.softmax(x)
    return out
def phi(obs,M, sin_cos = 1):
  x = obs[0]
  v = obs[1]

  M = M+1
  -0.7
  x1 = (x+1.2)/1.8
  v1 = (v+0.7)/(2*0.7) #(v+ np.pi/15)/(2*np.pi/15)#


  if(sin_cos == 0):
    vec = np.array([np.cos(np.pi*x1*np.arange(1,M)), np.cos(np.pi*v1*np.arange(1,M))])
  else:
    x1 = 2*x1-1
    v1 = 2*v1-1

    vec = np.array([np.sin(np.pi*x1*np.arange(1,M)), np.sin(np.pi*v1*np.arange(1,M))])


  phi_s = np.append(np.array([1]), vec.flatten())
  return phi_s




M = 5

if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
else:
    DEVICE = torch.device("cpu")
reward_list = []
no_of_actions = []
for i in range(20):
  value_model = Model(2*M+1,32,1).to(DEVICE)
  pi_ = Policy(2*M+1,32,3).to(DEVICE)


  done = False
  gm = 0.99
  i = gm
  count = 0
  optimizer_val = optim.SGD(value_model.parameters(), lr = 0.01)
  optimizer_policy = optim.SGD(pi_.parameters(), lr = 0.01)
  gamma = 0.5

  scheduler = StepLR(optimizer_val, step_size=100, gamma=gamma)
  scheduler1 = StepLR(optimizer_policy, step_size=100, gamma=gamma)




  rew = []
  list_itr = []
  timesteps = 0
  temp = 8.0
  while(True):
    done = False
    i = 1
    loss1 = 0
    loss2 = 0
    r1 = 0
    S0 = env.reset()
    S0 = np.array([0.0,0])
    env.state = env.unwrapped.state = S0
    s_curr = S0
    time_step = 0
    while not done:
      #m = Categorical(pi_(torch.tensor(s_curr.tolist())))
      s_curr1 = phi(s_curr,M, sin_cos = 0)
      m = torch.from_numpy(s_curr1).float().unsqueeze(0).to(DEVICE)
      state = Categorical(pi_(m, temp=temp))
      action = state.sample()

      log_prob = state.log_prob(action)[0]



      # probs = pi_(torch.tensor(s_curr.tolist()))
      # action = random.choices([0,1], weights=probs, k=1)[0]

      # log_prob = torch.log(probs[action])
      #action = torch.argmax(probs).item()
      s_next, reward, done, info = env.step(action.item())
      # print(action.item())
      s_next1 = phi(s_next,M, sin_cos = 0)

      m1 = torch.from_numpy(s_next1).float().unsqueeze(0).to(DEVICE)

      #delta = reward + gm*value_model(torch.tensor(s_next.tolist())) - value_model(torch.tensor(s_curr.tolist()))
      #loss_value =  F.mse_loss(reward + gm*value_model(torch.tensor(s_next.tolist())), value_model(torch.tensor(s_curr.tolist())))#i*delta**2
      delta = reward + gm*value_model(m1) - value_model(m)
      delta = delta.detach()
      timesteps += 1
      #print(m1)
      #print(value_model(torch.tensor(s_curr.tolist()).to(DEVICE)))
      # k = gm*value_model(torch.tensor(m1))
      # k.detach()
      # i*F.mse_loss(reward + k, value_model(torch.tensor(m))) #
      loss_value = -i*delta* value_model(m) #i*F.mse_loss(reward + value_model(torch.tensor(m1)), value_model(torch.tensor(m)))
      m.detach()
      m1.detach()


      optimizer_val.zero_grad()
      loss_value.backward()

      loss_policy = -i*delta*log_prob
      optimizer_policy.zero_grad()
      loss_policy.backward()

      optimizer_val.step()
      optimizer_policy.step()

      loss1 = loss_value + loss1
      loss2 = loss_policy + loss2

      i = i*gm
      s_curr = s_next
      #print(s_curr)
      #print(probs)
      r1 = r1+ reward

      #observation, reward, done, info = env.step(action)
    print("reward: ", r1, "loss_value: ", loss1, "loss_policy: ", loss2)
    rew.append(r1)
    list_itr.append(timesteps)

    scheduler.step()
    scheduler1.step()

    count = count+1
    print(count)
    if(count == 800):
      break;
    if count % 100 == 0:
      temp = temp/np.sqrt(2)
    #break;
  reward_list.append(rew)
  no_of_actions.append(list_itr)



avg_rew1 = np.mean(np.array(reward_list), axis = 0)
avg_no_of_actions1 = np.mean(np.array(no_of_actions), axis = 0)
print(avg_rew1.shape)

plt.figure(1)
plt.plot(avg_rew1)
plt.ylabel('Reward')
plt.xlabel('episodes')
plt.title('Average reward over 20 trails using One-step Actor-Critic')

plt.figure(2)
plt.plot(avg_no_of_actions1, range(avg_no_of_actions1.shape[0]))
plt.ylabel('episodes')
plt.xlabel('Timesteps')
plt.title('Avg timesteps taken for episodes, using One-step Actor-Critic')

plt.show()