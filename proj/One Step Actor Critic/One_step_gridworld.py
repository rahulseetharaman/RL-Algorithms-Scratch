# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p1Xwv-deXQST8-D8AuKjio2PUJPC3sN3
"""

import numpy as np
import gym
import matplotlib.pyplot as plt
from queue import PriorityQueue
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR
import gym
from torch.distributions import Categorical
NUM_STATES=25
NUM_ACTIONS=4
action_str = ['UP', "DOWN", "LEFT", "RIGHT"]
action_img = ['↑', '↓', '←', '→']
OBSTACLE_STATES = [12,17]
GAMMA = 0.9
WATER_STATES = [22]
REWARD_STATES = [24]
TERMINAL_STATES = REWARD_STATES
STEP_SIZE = 0.1
import random

class GridWorldEnv():
    def __init__(self):
        self.num_states = 25
        self.num_actions = 4
        self.current_state = 0
        self.possible = []
        for i in range(25):
            if i not in [12,17,24]:
                self.possible.append(i)

    def step(self, action):
        if self.current_state == 24:
            return self.current_state, 0, True, 2

        state_distribution = self._get_next_states(self.current_state, action)
        # sample from state distribution
        next_state = np.random.choice(range(25), p=state_distribution)
        self.current_state = next_state
        reward = self._get_reward(self.current_state)
        return self.current_state, reward, False, 2

    def reset(self):
        self.current_state = random.choice(self.possible)
        return self.current_state

    def _get_reward(self, state):
        if state in WATER_STATES:
            return -10
        if self._get_coords(state) == (4,4):
            return +10
        return 0

    def _get_coords(self, state):
        i=state//5
        j=state - i*5
        return (i,j)

    def _get_next_states_list(self, state):
        i = state//5
        j = state - 5*i

        up = (i-1)*5 + j if i > 0 else -1
        down = (i+1)*5 + j if i < 4 else -1
        left = i*5 + j-1 if j > 0 else -1
        right = i*5 + j+1 if j < 4 else -1
        next_s = [up, down, left, right]
        next_s = [-1 if n in OBSTACLE_STATES else n for n in next_s]
        return next_s

    def _get_next_states(self, state, action):
        next_s = self._get_next_states_list(state)
        if action == 0:
            probs = [0.8, 0.0, 0.05, 0.05]
        elif action == 1:
            probs = [0.0, 0.8, 0.05, 0.05]
        elif action == 2:
            probs = [0.05, 0.05, 0.8, 0.0]
        else:
            probs = [0.05, 0.05, 0.0, 0.8]
        state_dist = [0 for _ in range(NUM_STATES)]
        state_dist[state] = 0.1
        for (s,p) in zip(next_s, probs):
            if s == -1:
                # if it is an obstacle, it will hit it and come back to same state
                state_dist[state] += p
            else:
                state_dist[s] += p
        return state_dist

obstacles = [(2,2), (3,2)]
def print_policy(pi, terminal_states):
  #print(pi)
  dim1, dim2 = pi.shape
  arrow = ["\u2191", "\u2193", "\u2190", "\u2192"]
  print("Policy:")
  for i in range(dim1):
    str1 = ""
    for j in range(dim2):
      if((i,j) in obstacles):
        str1 = str1 + "  "
      elif((i,j) in terminal_states):
        str1 = str1 + "G" + " "
      else:
        str1 = str1 + arrow[int(pi[i][j])] + " "
    print(str1)

state_param = np.eye(NUM_STATES)
env = GridWorldEnv()
expected_val = np.array([
    [4.01868852, 4.55478372, 5.15754448, 5.83363576, 6.45528788],
    [4.37160459, 5.03235836, 5.80129547, 6.64726542, 7.39070891],
    [3.86716748, 4.38996431, 0., 7.57690464, 8.46366148],
    [3.41824855, 3.83189595, 0., 8.5738302, 9.69459232],
    [2.9976879, 2.93092658, 6.07330058, 9.69459232, 0.]
])
final_val = expected_val.reshape(25,1)

value_list = []
policy_list= []

rew_list = []
if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
else:
    DEVICE = torch.device("cpu")
def softmax(x):
      exp_x = np.exp(x - np.max(x))  # Subtracting the maximum value for numerical stability
      return exp_x / exp_x.sum(axis=0)
avg_mse = []
no_of_actions = []

for l in range(20):
  value_fun = np.random.rand(25,1)
  value_fun[12] = [0]
  value_fun[17] = [0]
  value_fun[24] = [0]
  policy = np.random.rand(25,4)
  list_state = []
  list_itr = []
  rew = []
  mse_ls = []
  alpha = 0.5
  alpha1 = 0.5
  gmm = 0.9
  gm = 0.9
  count = 0
  time_step = 0

  while(True):
    done = False
    i = 1
    loss1 = 0
    loss2 = 0
    r1 = 0
    S0 = env.reset()
    s_curr = S0
    while not done:
      if(s_curr == 24):
        break

      list_state.append(s_curr)


      # if(count % 200==1):
      #   if(temp>1):
      #     temp = temp/1.2
      #   else:
      #     temp =1

      scores = softmax(policy[s_curr])
      state = Categorical(torch.from_numpy(scores).float())
      action = state.sample()
      log_prob = state.log_prob(action)
      action =action.item()

      s_next, reward, done, info = env.step(action)

      # s_next1 = state_param[s_next]
      # m1 = torch.from_numpy(s_next1).float().to(DEVICE)

      delta = 0
      if(s_next == 24):
        delta = 10 -value_fun[s_curr]
      else:
        delta = reward + gm*value_fun[s_next] - value_fun[s_curr]


      value_fun[s_curr] += alpha*i*delta
      for j in range(4):
        if(j == action):
          policy[s_curr][j] += alpha1*i*delta#(1-scores[action])
        #else:
          #policy[s_curr][j] += alpha1*i*delta#(-scores[action])




      loss_value =  -i*delta* value_fun[s_curr]
      loss_policy = -i*delta*log_prob.item()

      loss1 += loss_value
      loss2 += loss_policy

      i = i*gmm
      s_curr = s_next


      r1 = r1+ reward
      time_step += 1
    if(count%500 == 0):
      print("reward: ", r1, "loss_value: ", loss1, "loss_policy: ", loss2)
    rew.append(r1)

    list_itr.append(time_step)
    #print(np.mean((value_fun - final_val) ** 2))
    mse_ls.append(np.mean((value_fun - final_val) ** 2))

    count = count+1
    #print(count)
    if(count%500 ==0):
      alpha = alpha/2
      alpha1 = alpha1/2
    if(count == 1000):
      break
  rew_list.append(rew)
  value_list.append(value_fun)
  policy_list.append(policy)
  avg_mse.append(mse_ls)
  no_of_actions.append(list_itr)

avg = np.array(policy_list)
# print(avg.shape)
policy = np.mean(np.array(policy_list), axis = 0)
mse_loss_avg = np.mean(np.array(avg_mse), axis = 0)
# print(policy)
print_policy(np.argmax(np.array(policy), axis = 1).reshape(5,5), [(4,4)])
expected = np.array([
    [4.01868852, 4.55478372, 5.15754448, 5.83363576, 6.45528788],
    [4.37160459, 5.03235836, 5.80129547, 6.64726542, 7.39070891],
    [3.86716748, 4.38996431, 0., 7.57690464, 8.46366148],
    [3.41824855, 3.83189595, 0., 8.5738302, 9.69459232],
    [2.9976879, 2.93092658, 6.07330058, 9.69459232, 0.]
])
print("value function")
value_fn = np.mean(np.array(value_list), axis =0)
print(value_fn.reshape(5,5))
avg_no_of_actions = np.mean(np.array(no_of_actions), axis = 0)

# for k in range(20):
#   print_policy(np.argmax(np.array(policy_list[k]), axis = 1).reshape(5,5), [(4,4)])
# np.max(abs(value_fun.reshape(5,5) - expected))
plt.figure(10)
plt.plot(mse_loss_avg)
plt.ylabel('mse')
plt.xlabel('episodes')
plt.title('MSE over episodes')

plt.figure(2)
plt.plot(avg_no_of_actions, range(avg_no_of_actions.shape[0]))
plt.ylabel('episodes')
plt.xlabel('Timesteps')
plt.title('Avg Timesteps taken for episodes, using One-step Actor-Critic')
plt.show()